{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import torch\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the env variables\n",
    "NAME_COMPANY = \"processed_data_YesBank_StockPrices.csv\"\n",
    "PATH_PROCESS_INFO = f\"../data/processed/{NAME_COMPANY}\"\n",
    "PATH_MODEL = f\"../models/model_lstm.pkl\"\n",
    "BOX_COX_TRANSFORMER_PATH = \"../config/model/transformers_boxcox.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data transform to start the training process\n",
    "dataset_processed = pd.read_csv(PATH_PROCESS_INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.613965</td>\n",
       "      <td>2.682707</td>\n",
       "      <td>2.493722</td>\n",
       "      <td>2.568818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.579869</td>\n",
       "      <td>2.745719</td>\n",
       "      <td>2.609925</td>\n",
       "      <td>2.645822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.651627</td>\n",
       "      <td>2.745024</td>\n",
       "      <td>2.585922</td>\n",
       "      <td>2.636499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.629821</td>\n",
       "      <td>2.716832</td>\n",
       "      <td>2.597132</td>\n",
       "      <td>2.612025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.641559</td>\n",
       "      <td>2.673812</td>\n",
       "      <td>2.637552</td>\n",
       "      <td>2.645049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Open      High       Low     Close\n",
       "0  2.613965  2.682707  2.493722  2.568818\n",
       "1  2.579869  2.745719  2.609925  2.645822\n",
       "2  2.651627  2.745024  2.585922  2.636499\n",
       "3  2.629821  2.716832  2.597132  2.612025\n",
       "4  2.641559  2.673812  2.637552  2.645049"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.61396481, 2.68270663, 2.49372188, 2.56881781],\n",
       "       [2.57986883, 2.74571874, 2.60992489, 2.64582239],\n",
       "       [2.65162713, 2.74502359, 2.58592243, 2.63649926],\n",
       "       [2.62982127, 2.71683211, 2.59713216, 2.61202534],\n",
       "       [2.64155903, 2.67381241, 2.63755175, 2.6450486 ],\n",
       "       [2.65239763, 2.71468682, 2.64742699, 2.66801832],\n",
       "       [2.6669312 , 2.89326531, 2.69393755, 2.78408128],\n",
       "       [2.79685703, 2.8817327 , 2.82823193, 2.83636029],\n",
       "       [2.84286095, 3.1002372 , 2.87046334, 3.06534175],\n",
       "       [3.09160247, 3.09277568, 2.99657994, 3.03421148],\n",
       "       [3.0522345 , 3.14154742, 2.8556634 , 2.95299616],\n",
       "       [2.96424459, 2.96556888, 2.679714  , 2.80357439],\n",
       "       [2.82991999, 2.89567715, 2.71261887, 2.8389397 ],\n",
       "       [2.82338962, 2.97677804, 2.84886959, 2.94195337],\n",
       "       [2.95271375, 2.9922725 , 2.92138415, 2.97757004],\n",
       "       [2.99699127, 3.25012838, 3.02481144, 3.19717919],\n",
       "       [3.19980204, 3.41934506, 3.18152504, 3.30783349],\n",
       "       [3.3129648 , 3.46204029, 3.32319207, 3.373996  ],\n",
       "       [3.39249765, 3.60948856, 3.43284263, 3.49130383],\n",
       "       [3.49472654, 3.56066782, 3.4247818 , 3.43664995],\n",
       "       [3.46292351, 3.52748275, 3.2737907 , 3.41853627],\n",
       "       [3.37705909, 3.54094732, 3.40024243, 3.49651416],\n",
       "       [3.51062258, 3.66837555, 3.51450418, 3.60200126],\n",
       "       [3.61603513, 3.67529576, 3.56163128, 3.67684142],\n",
       "       [3.67909013, 3.80141012, 3.71574574, 3.73243135],\n",
       "       [3.72272103, 3.74803845, 3.55445403, 3.7059949 ],\n",
       "       [3.71990459, 3.81937268, 3.72443667, 3.82370505],\n",
       "       [3.83731409, 3.95019495, 3.67059952, 3.87645849],\n",
       "       [3.90109259, 4.02882396, 3.86161883, 3.94008902],\n",
       "       [3.94769672, 4.08927084, 3.99020561, 4.02008742],\n",
       "       [4.02659224, 4.1191995 , 3.55589318, 4.03547918],\n",
       "       [4.04528712, 4.08868696, 3.97547013, 4.01177306],\n",
       "       [4.00324432, 3.98509852, 3.31026208, 3.60950769],\n",
       "       [3.61634586, 3.65849383, 3.49188617, 3.61912601],\n",
       "       [3.62870144, 3.6895692 , 3.54831683, 3.5245556 ],\n",
       "       [3.52966011, 3.54842619, 3.24133972, 3.20038844],\n",
       "       [3.2020958 , 3.49117141, 3.12996151, 3.31319835],\n",
       "       [3.29631963, 3.43785483, 3.33290142, 3.36853931],\n",
       "       [3.36454512, 3.46695162, 3.19653628, 3.25742314],\n",
       "       [3.27940996, 3.28396894, 2.47084361, 2.65812621],\n",
       "       [2.74805554, 2.9312048 , 2.49372188, 2.55204062],\n",
       "       [2.56491157, 2.8564058 , 2.4823429 , 2.76352972],\n",
       "       [2.78197507, 2.96218266, 2.51520447, 2.55034809],\n",
       "       [2.54718866, 2.56569076, 2.3582752 , 2.36765415],\n",
       "       [2.34203488, 2.45613333, 2.15427849, 2.33904832],\n",
       "       [2.34616458, 2.91418326, 2.36254113, 2.79219111],\n",
       "       [2.82600661, 3.43280595, 2.78286173, 3.29952661],\n",
       "       [3.32119078, 3.51082693, 3.25932882, 3.47445746],\n",
       "       [3.47717536, 3.57845363, 3.3420908 , 3.55122011],\n",
       "       [3.5549655 , 3.64023323, 3.5103918 , 3.60011637],\n",
       "       [3.61323452, 3.80802332, 3.61744462, 3.81422563],\n",
       "       [3.82204131, 4.05647104, 3.76987976, 3.96563763],\n",
       "       [3.95179354, 4.10817024, 3.93622939, 4.03610702],\n",
       "       [4.0473675 , 4.12127842, 4.07478549, 4.09393944],\n",
       "       [4.10048725, 4.15674341, 4.02432989, 4.02072433],\n",
       "       [4.00540758, 4.03907208, 3.98040343, 3.96586103],\n",
       "       [3.97915645, 4.05083745, 4.04496992, 4.04444333],\n",
       "       [4.04090518, 4.19088823, 4.07930332, 4.16471123],\n",
       "       [4.16461189, 4.16583557, 4.12047462, 4.17340001],\n",
       "       [4.16012933, 4.19829055, 4.16187459, 4.10223383],\n",
       "       [4.09949768, 4.21640172, 4.17064111, 4.19923272],\n",
       "       [4.2095138 , 4.34537826, 4.28879334, 4.25578095],\n",
       "       [4.26864637, 4.38826347, 4.3525247 , 4.38545967],\n",
       "       [4.39769645, 4.44910854, 4.44921736, 4.408795  ],\n",
       "       [4.41704435, 4.4710947 , 4.26971579, 4.23841583],\n",
       "       [4.2515885 , 4.33739237, 4.21987001, 4.26122485],\n",
       "       [4.27168834, 4.26037705, 4.09252644, 4.07755869],\n",
       "       [4.0885519 , 4.15875035, 4.03117358, 4.05106609],\n",
       "       [4.06509456, 4.28753348, 4.1235136 , 4.25167989],\n",
       "       [4.25502173, 4.33584975, 4.31636374, 4.23511759],\n",
       "       [4.24226383, 4.2291036 , 4.20100176, 4.21847303],\n",
       "       [4.2272449 , 4.25788873, 4.2070578 , 4.25850634],\n",
       "       [4.27202581, 4.33182846, 4.33197692, 4.25424488],\n",
       "       [4.27033742, 4.26731359, 4.11720947, 4.13669246],\n",
       "       [4.14694762, 4.20144729, 4.13215158, 4.11533423],\n",
       "       [4.10443646, 4.28105273, 4.06341109, 4.26731652],\n",
       "       [4.25844406, 4.26302487, 4.15007907, 4.11513987],\n",
       "       [4.16572962, 4.19653275, 4.04729163, 3.97475902],\n",
       "       [3.98181061, 4.30832797, 4.01696795, 4.31823303],\n",
       "       [4.3198742 , 4.43231682, 4.37166285, 4.36777449],\n",
       "       [4.36878061, 4.47489619, 4.41605177, 4.43221564],\n",
       "       [4.43778982, 4.45049601, 4.46375972, 4.3819761 ],\n",
       "       [4.39016173, 4.37122598, 4.28579249, 4.31871575],\n",
       "       [4.3125871 , 4.37137531, 4.3459542 , 4.34744735],\n",
       "       [4.35558968, 4.40936511, 4.43876179, 4.42334638],\n",
       "       [4.42878086, 4.43203471, 4.40037273, 4.31791109],\n",
       "       [4.32470508, 4.47231808, 4.3865382 , 4.47452645],\n",
       "       [4.47728685, 4.55729071, 4.56346226, 4.55305497],\n",
       "       [4.55972643, 4.62298864, 4.65690497, 4.63036926],\n",
       "       [4.64257785, 4.68488694, 4.73280026, 4.68168983],\n",
       "       [4.68945619, 4.81084098, 4.79679983, 4.80714799],\n",
       "       [4.81097227, 4.81873446, 4.79919595, 4.70060722],\n",
       "       [4.69859357, 4.74076379, 4.67305387, 4.59645279],\n",
       "       [4.59982791, 4.75806706, 4.67318749, 4.7640123 ],\n",
       "       [4.77531954, 4.83463996, 4.84379567, 4.73386955],\n",
       "       [4.75402266, 4.76986101, 4.72184098, 4.67430719],\n",
       "       [4.68024121, 4.74340352, 4.26417817, 4.29825267],\n",
       "       [4.3214869 , 4.3288802 , 3.94591492, 3.99516591],\n",
       "       [4.0097211 , 4.48916632, 3.98531519, 4.17211037],\n",
       "       [4.18418973, 4.45519997, 4.23253875, 4.43655098],\n",
       "       [4.44327457, 4.46659693, 4.3897985 , 4.4362625 ],\n",
       "       [4.44901808, 4.5421267 , 4.50351643, 4.44029424],\n",
       "       [4.4538762 , 4.45395682, 4.32183465, 4.24498216],\n",
       "       [4.24710891, 4.24636707, 4.27768809, 4.23389989],\n",
       "       [4.22935317, 4.54669852, 4.3134351 , 4.55835046],\n",
       "       [4.57142763, 4.68466416, 4.63749578, 4.62626302],\n",
       "       [4.63777661, 4.91099399, 4.71966213, 4.90011941],\n",
       "       [4.91262457, 4.91072332, 4.89358053, 4.84705567],\n",
       "       [4.85143717, 4.88228734, 4.88049941, 4.84537652],\n",
       "       [4.84121195, 4.92078231, 4.91205958, 4.90424708],\n",
       "       [4.91075487, 4.99135814, 4.9532884 , 4.87933823],\n",
       "       [4.88710853, 5.07932524, 4.97423507, 5.09681139],\n",
       "       [5.10322416, 5.12545453, 5.19125936, 5.13723197],\n",
       "       [5.14159304, 5.2044454 , 5.20446372, 5.22776595],\n",
       "       [5.23109511, 5.35783213, 5.33538519, 5.34585714],\n",
       "       [5.35057521, 5.32688954, 5.37694839, 5.34616923],\n",
       "       [5.36486459, 5.37555893, 5.38201356, 5.28692607],\n",
       "       [5.28817068, 5.32024776, 5.36550688, 5.31714159],\n",
       "       [5.33482878, 5.34560952, 5.38921139, 5.37066857],\n",
       "       [5.3783604 , 5.36021259, 5.39707779, 5.3212356 ],\n",
       "       [5.32846623, 5.33726211, 5.37948374, 5.30282037],\n",
       "       [5.30850394, 5.29774557, 5.06128582, 5.10517209],\n",
       "       [5.09147712, 5.20787389, 5.14399576, 5.16587516],\n",
       "       [5.17792702, 5.21088198, 5.26775705, 5.2084679 ],\n",
       "       [5.21231086, 5.2245851 , 5.28594021, 5.21889617],\n",
       "       [5.22128596, 5.20114375, 5.20202045, 5.16078351],\n",
       "       [5.16347881, 5.16883724, 5.13910782, 5.19105459],\n",
       "       [5.20283204, 5.22384336, 5.19381041, 5.10338034],\n",
       "       [5.11647417, 5.33446524, 5.24050654, 5.34841361],\n",
       "       [5.34756941, 5.42392418, 5.442024  , 5.44275022],\n",
       "       [5.45374564, 5.52323227, 5.54654587, 5.53926118],\n",
       "       [5.54437486, 5.59253761, 5.66481067, 5.6146049 ],\n",
       "       [5.6230273 , 5.7026909 , 5.77399794, 5.71772285],\n",
       "       [5.72560248, 5.81696153, 5.86968704, 5.84009753],\n",
       "       [5.84866056, 5.87392039, 5.82373398, 5.75017907],\n",
       "       [5.76664229, 5.77927801, 5.89377648, 5.76164269],\n",
       "       [5.77342814, 5.76799852, 5.76233425, 5.6762283 ],\n",
       "       [5.68670782, 5.69320648, 5.75789753, 5.66219656],\n",
       "       [5.67084994, 5.85266417, 5.79925618, 5.86580394],\n",
       "       [5.87651997, 5.88931134, 6.02499186, 5.9094497 ],\n",
       "       [5.91535025, 5.95092252, 6.05780835, 5.9791155 ],\n",
       "       [5.98422809, 6.01454019, 6.12319776, 6.03488372],\n",
       "       [6.04041051, 6.01362965, 6.02307441, 5.89372986],\n",
       "       [5.90051269, 5.932747  , 6.04399653, 5.91618758],\n",
       "       [5.92443992, 6.14148082, 6.08653759, 6.14821595],\n",
       "       [6.1575634 , 6.13136687, 6.25766899, 6.11522311],\n",
       "       [6.12091015, 6.173549  , 6.28514756, 6.11165449],\n",
       "       [6.13203416, 6.1522754 , 6.11859055, 5.99488032],\n",
       "       [6.00477067, 6.01200179, 6.10699314, 5.96847889],\n",
       "       [5.97576121, 5.97623209, 6.12492083, 5.99729639],\n",
       "       [5.99958409, 6.12398393, 6.15028215, 6.12539989],\n",
       "       [6.13326329, 6.12545278, 6.14012741, 6.02184478],\n",
       "       [6.02256457, 5.9988905 , 6.06386846, 5.96173397],\n",
       "       [5.96794246, 6.13203823, 6.13501708, 6.14848654],\n",
       "       [6.15711264, 6.11927024, 6.21707491, 6.09977981],\n",
       "       [6.11155393, 6.07621033, 6.22289975, 6.0788467 ],\n",
       "       [6.08621005, 6.20155681, 6.24072442, 6.16593413],\n",
       "       [6.17824956, 6.23033073, 6.25982129, 6.09094755],\n",
       "       [6.10904551, 6.07590314, 5.44763284, 5.41216105],\n",
       "       [5.39649981, 5.79112041, 5.46423153, 5.43885837],\n",
       "       [5.46619597, 5.63208655, 5.30900565, 5.32791926],\n",
       "       [5.3599155 , 5.44534241, 5.40630625, 5.40212735],\n",
       "       [5.41345843, 5.65450703, 5.54573082, 5.47411846],\n",
       "       [5.49397726, 5.65722078, 5.45782341, 5.66191596],\n",
       "       [5.68301881, 5.84124912, 5.81180875, 5.84995579],\n",
       "       [5.86326665, 5.85864596, 5.44181828, 5.31739792],\n",
       "       [5.29147591, 5.35215744, 5.19644058, 5.18071848],\n",
       "       [5.18515283, 5.19617224, 4.86150109, 4.85020939],\n",
       "       [4.86293678, 4.84591319, 4.65051725, 4.66383944],\n",
       "       [4.64257785, 4.65553024, 4.17334083, 4.20836131],\n",
       "       [4.21130009, 4.40010265, 3.88677622, 3.82599822],\n",
       "       [3.84235705, 4.45809504, 3.51226295, 4.38772552],\n",
       "       [4.39919711, 4.4400458 , 4.30626467, 4.35478446],\n",
       "       [4.3840959 , 4.36988107, 3.88006869, 3.95756413],\n",
       "       [3.95903841, 3.97647954, 3.76243285, 3.76714106],\n",
       "       [3.76680471, 3.79269937, 3.67059952, 3.63415457],\n",
       "       [3.65599026, 4.60333871, 1.75036131, 3.18191777],\n",
       "       [3.176588  , 3.48945972, 3.12467022, 3.41142883],\n",
       "       [3.40771572, 3.52814355, 3.35809869, 3.36932055],\n",
       "       [3.4058253 , 3.53964123, 3.37303167, 3.31935511],\n",
       "       [3.32119078, 3.41310229, 2.47947945, 2.52549529],\n",
       "       [2.53089248, 2.89326531, 2.54889778, 2.71685533],\n",
       "       [2.71301018, 2.77720689, 2.62675179, 2.62472806],\n",
       "       [2.63766095, 2.68344441, 2.57196571, 2.56548371],\n",
       "       [2.56574813, 2.74710766, 2.58070958, 2.73832738]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir el DataFrame a un NumPy array\n",
    "data_numpy = dataset_processed.values\n",
    "data_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6140, 2.6827, 2.4937, 2.5688],\n",
       "        [2.5799, 2.7457, 2.6099, 2.6458],\n",
       "        [2.6516, 2.7450, 2.5859, 2.6365],\n",
       "        [2.6298, 2.7168, 2.5971, 2.6120],\n",
       "        [2.6416, 2.6738, 2.6376, 2.6450],\n",
       "        [2.6524, 2.7147, 2.6474, 2.6680],\n",
       "        [2.6669, 2.8933, 2.6939, 2.7841],\n",
       "        [2.7969, 2.8817, 2.8282, 2.8364],\n",
       "        [2.8429, 3.1002, 2.8705, 3.0653],\n",
       "        [3.0916, 3.0928, 2.9966, 3.0342],\n",
       "        [3.0522, 3.1415, 2.8557, 2.9530],\n",
       "        [2.9642, 2.9656, 2.6797, 2.8036],\n",
       "        [2.8299, 2.8957, 2.7126, 2.8389],\n",
       "        [2.8234, 2.9768, 2.8489, 2.9420],\n",
       "        [2.9527, 2.9923, 2.9214, 2.9776],\n",
       "        [2.9970, 3.2501, 3.0248, 3.1972],\n",
       "        [3.1998, 3.4193, 3.1815, 3.3078],\n",
       "        [3.3130, 3.4620, 3.3232, 3.3740],\n",
       "        [3.3925, 3.6095, 3.4328, 3.4913],\n",
       "        [3.4947, 3.5607, 3.4248, 3.4367],\n",
       "        [3.4629, 3.5275, 3.2738, 3.4185],\n",
       "        [3.3771, 3.5409, 3.4002, 3.4965],\n",
       "        [3.5106, 3.6684, 3.5145, 3.6020],\n",
       "        [3.6160, 3.6753, 3.5616, 3.6768],\n",
       "        [3.6791, 3.8014, 3.7157, 3.7324],\n",
       "        [3.7227, 3.7480, 3.5545, 3.7060],\n",
       "        [3.7199, 3.8194, 3.7244, 3.8237],\n",
       "        [3.8373, 3.9502, 3.6706, 3.8765],\n",
       "        [3.9011, 4.0288, 3.8616, 3.9401],\n",
       "        [3.9477, 4.0893, 3.9902, 4.0201],\n",
       "        [4.0266, 4.1192, 3.5559, 4.0355],\n",
       "        [4.0453, 4.0887, 3.9755, 4.0118],\n",
       "        [4.0032, 3.9851, 3.3103, 3.6095],\n",
       "        [3.6163, 3.6585, 3.4919, 3.6191],\n",
       "        [3.6287, 3.6896, 3.5483, 3.5246],\n",
       "        [3.5297, 3.5484, 3.2413, 3.2004],\n",
       "        [3.2021, 3.4912, 3.1300, 3.3132],\n",
       "        [3.2963, 3.4379, 3.3329, 3.3685],\n",
       "        [3.3645, 3.4670, 3.1965, 3.2574],\n",
       "        [3.2794, 3.2840, 2.4708, 2.6581],\n",
       "        [2.7481, 2.9312, 2.4937, 2.5520],\n",
       "        [2.5649, 2.8564, 2.4823, 2.7635],\n",
       "        [2.7820, 2.9622, 2.5152, 2.5503],\n",
       "        [2.5472, 2.5657, 2.3583, 2.3677],\n",
       "        [2.3420, 2.4561, 2.1543, 2.3390],\n",
       "        [2.3462, 2.9142, 2.3625, 2.7922],\n",
       "        [2.8260, 3.4328, 2.7829, 3.2995],\n",
       "        [3.3212, 3.5108, 3.2593, 3.4745],\n",
       "        [3.4772, 3.5785, 3.3421, 3.5512],\n",
       "        [3.5550, 3.6402, 3.5104, 3.6001],\n",
       "        [3.6132, 3.8080, 3.6174, 3.8142],\n",
       "        [3.8220, 4.0565, 3.7699, 3.9656],\n",
       "        [3.9518, 4.1082, 3.9362, 4.0361],\n",
       "        [4.0474, 4.1213, 4.0748, 4.0939],\n",
       "        [4.1005, 4.1567, 4.0243, 4.0207],\n",
       "        [4.0054, 4.0391, 3.9804, 3.9659],\n",
       "        [3.9792, 4.0508, 4.0450, 4.0444],\n",
       "        [4.0409, 4.1909, 4.0793, 4.1647],\n",
       "        [4.1646, 4.1658, 4.1205, 4.1734],\n",
       "        [4.1601, 4.1983, 4.1619, 4.1022],\n",
       "        [4.0995, 4.2164, 4.1706, 4.1992],\n",
       "        [4.2095, 4.3454, 4.2888, 4.2558],\n",
       "        [4.2686, 4.3883, 4.3525, 4.3855],\n",
       "        [4.3977, 4.4491, 4.4492, 4.4088],\n",
       "        [4.4170, 4.4711, 4.2697, 4.2384],\n",
       "        [4.2516, 4.3374, 4.2199, 4.2612],\n",
       "        [4.2717, 4.2604, 4.0925, 4.0776],\n",
       "        [4.0886, 4.1588, 4.0312, 4.0511],\n",
       "        [4.0651, 4.2875, 4.1235, 4.2517],\n",
       "        [4.2550, 4.3358, 4.3164, 4.2351],\n",
       "        [4.2423, 4.2291, 4.2010, 4.2185],\n",
       "        [4.2272, 4.2579, 4.2071, 4.2585],\n",
       "        [4.2720, 4.3318, 4.3320, 4.2542],\n",
       "        [4.2703, 4.2673, 4.1172, 4.1367],\n",
       "        [4.1469, 4.2014, 4.1322, 4.1153],\n",
       "        [4.1044, 4.2811, 4.0634, 4.2673],\n",
       "        [4.2584, 4.2630, 4.1501, 4.1151],\n",
       "        [4.1657, 4.1965, 4.0473, 3.9748],\n",
       "        [3.9818, 4.3083, 4.0170, 4.3182],\n",
       "        [4.3199, 4.4323, 4.3717, 4.3678],\n",
       "        [4.3688, 4.4749, 4.4161, 4.4322],\n",
       "        [4.4378, 4.4505, 4.4638, 4.3820],\n",
       "        [4.3902, 4.3712, 4.2858, 4.3187],\n",
       "        [4.3126, 4.3714, 4.3460, 4.3474],\n",
       "        [4.3556, 4.4094, 4.4388, 4.4233],\n",
       "        [4.4288, 4.4320, 4.4004, 4.3179],\n",
       "        [4.3247, 4.4723, 4.3865, 4.4745],\n",
       "        [4.4773, 4.5573, 4.5635, 4.5531],\n",
       "        [4.5597, 4.6230, 4.6569, 4.6304],\n",
       "        [4.6426, 4.6849, 4.7328, 4.6817],\n",
       "        [4.6895, 4.8108, 4.7968, 4.8071],\n",
       "        [4.8110, 4.8187, 4.7992, 4.7006],\n",
       "        [4.6986, 4.7408, 4.6731, 4.5965],\n",
       "        [4.5998, 4.7581, 4.6732, 4.7640],\n",
       "        [4.7753, 4.8346, 4.8438, 4.7339],\n",
       "        [4.7540, 4.7699, 4.7218, 4.6743],\n",
       "        [4.6802, 4.7434, 4.2642, 4.2983],\n",
       "        [4.3215, 4.3289, 3.9459, 3.9952],\n",
       "        [4.0097, 4.4892, 3.9853, 4.1721],\n",
       "        [4.1842, 4.4552, 4.2325, 4.4366],\n",
       "        [4.4433, 4.4666, 4.3898, 4.4363],\n",
       "        [4.4490, 4.5421, 4.5035, 4.4403],\n",
       "        [4.4539, 4.4540, 4.3218, 4.2450],\n",
       "        [4.2471, 4.2464, 4.2777, 4.2339],\n",
       "        [4.2294, 4.5467, 4.3134, 4.5584],\n",
       "        [4.5714, 4.6847, 4.6375, 4.6263],\n",
       "        [4.6378, 4.9110, 4.7197, 4.9001],\n",
       "        [4.9126, 4.9107, 4.8936, 4.8471],\n",
       "        [4.8514, 4.8823, 4.8805, 4.8454],\n",
       "        [4.8412, 4.9208, 4.9121, 4.9042],\n",
       "        [4.9108, 4.9914, 4.9533, 4.8793],\n",
       "        [4.8871, 5.0793, 4.9742, 5.0968],\n",
       "        [5.1032, 5.1255, 5.1913, 5.1372],\n",
       "        [5.1416, 5.2044, 5.2045, 5.2278],\n",
       "        [5.2311, 5.3578, 5.3354, 5.3459],\n",
       "        [5.3506, 5.3269, 5.3769, 5.3462],\n",
       "        [5.3649, 5.3756, 5.3820, 5.2869],\n",
       "        [5.2882, 5.3202, 5.3655, 5.3171],\n",
       "        [5.3348, 5.3456, 5.3892, 5.3707],\n",
       "        [5.3784, 5.3602, 5.3971, 5.3212],\n",
       "        [5.3285, 5.3373, 5.3795, 5.3028],\n",
       "        [5.3085, 5.2977, 5.0613, 5.1052],\n",
       "        [5.0915, 5.2079, 5.1440, 5.1659],\n",
       "        [5.1779, 5.2109, 5.2678, 5.2085],\n",
       "        [5.2123, 5.2246, 5.2859, 5.2189],\n",
       "        [5.2213, 5.2011, 5.2020, 5.1608],\n",
       "        [5.1635, 5.1688, 5.1391, 5.1911],\n",
       "        [5.2028, 5.2238, 5.1938, 5.1034],\n",
       "        [5.1165, 5.3345, 5.2405, 5.3484],\n",
       "        [5.3476, 5.4239, 5.4420, 5.4428],\n",
       "        [5.4537, 5.5232, 5.5465, 5.5393],\n",
       "        [5.5444, 5.5925, 5.6648, 5.6146],\n",
       "        [5.6230, 5.7027, 5.7740, 5.7177],\n",
       "        [5.7256, 5.8170, 5.8697, 5.8401],\n",
       "        [5.8487, 5.8739, 5.8237, 5.7502],\n",
       "        [5.7666, 5.7793, 5.8938, 5.7616],\n",
       "        [5.7734, 5.7680, 5.7623, 5.6762],\n",
       "        [5.6867, 5.6932, 5.7579, 5.6622],\n",
       "        [5.6708, 5.8527, 5.7993, 5.8658],\n",
       "        [5.8765, 5.8893, 6.0250, 5.9094],\n",
       "        [5.9154, 5.9509, 6.0578, 5.9791],\n",
       "        [5.9842, 6.0145, 6.1232, 6.0349],\n",
       "        [6.0404, 6.0136, 6.0231, 5.8937],\n",
       "        [5.9005, 5.9327, 6.0440, 5.9162],\n",
       "        [5.9244, 6.1415, 6.0865, 6.1482],\n",
       "        [6.1576, 6.1314, 6.2577, 6.1152],\n",
       "        [6.1209, 6.1735, 6.2851, 6.1117],\n",
       "        [6.1320, 6.1523, 6.1186, 5.9949],\n",
       "        [6.0048, 6.0120, 6.1070, 5.9685],\n",
       "        [5.9758, 5.9762, 6.1249, 5.9973],\n",
       "        [5.9996, 6.1240, 6.1503, 6.1254],\n",
       "        [6.1333, 6.1255, 6.1401, 6.0218],\n",
       "        [6.0226, 5.9989, 6.0639, 5.9617],\n",
       "        [5.9679, 6.1320, 6.1350, 6.1485],\n",
       "        [6.1571, 6.1193, 6.2171, 6.0998],\n",
       "        [6.1116, 6.0762, 6.2229, 6.0788],\n",
       "        [6.0862, 6.2016, 6.2407, 6.1659],\n",
       "        [6.1782, 6.2303, 6.2598, 6.0909],\n",
       "        [6.1090, 6.0759, 5.4476, 5.4122],\n",
       "        [5.3965, 5.7911, 5.4642, 5.4389],\n",
       "        [5.4662, 5.6321, 5.3090, 5.3279],\n",
       "        [5.3599, 5.4453, 5.4063, 5.4021],\n",
       "        [5.4135, 5.6545, 5.5457, 5.4741],\n",
       "        [5.4940, 5.6572, 5.4578, 5.6619],\n",
       "        [5.6830, 5.8412, 5.8118, 5.8500],\n",
       "        [5.8633, 5.8586, 5.4418, 5.3174],\n",
       "        [5.2915, 5.3522, 5.1964, 5.1807],\n",
       "        [5.1852, 5.1962, 4.8615, 4.8502],\n",
       "        [4.8629, 4.8459, 4.6505, 4.6638],\n",
       "        [4.6426, 4.6555, 4.1733, 4.2084],\n",
       "        [4.2113, 4.4001, 3.8868, 3.8260],\n",
       "        [3.8424, 4.4581, 3.5123, 4.3877],\n",
       "        [4.3992, 4.4400, 4.3063, 4.3548],\n",
       "        [4.3841, 4.3699, 3.8801, 3.9576],\n",
       "        [3.9590, 3.9765, 3.7624, 3.7671],\n",
       "        [3.7668, 3.7927, 3.6706, 3.6342],\n",
       "        [3.6560, 4.6033, 1.7504, 3.1819],\n",
       "        [3.1766, 3.4895, 3.1247, 3.4114],\n",
       "        [3.4077, 3.5281, 3.3581, 3.3693],\n",
       "        [3.4058, 3.5396, 3.3730, 3.3194],\n",
       "        [3.3212, 3.4131, 2.4795, 2.5255],\n",
       "        [2.5309, 2.8933, 2.5489, 2.7169],\n",
       "        [2.7130, 2.7772, 2.6268, 2.6247],\n",
       "        [2.6377, 2.6834, 2.5720, 2.5655],\n",
       "        [2.5657, 2.7471, 2.5807, 2.7383]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir el NumPy array a tensor\n",
    "data_tensor = torch.tensor(data_numpy, dtype=torch.float32)\n",
    "data_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicializar los estados ocultos y las celdas\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Pasar la entrada por la LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Pasar la salida de la LSTM por la capa lineal\n",
    "        out = self.fc(out[:, -1, :])  # Tomar la última salida de la secuencia\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Parámetros\n",
    "input_size = 4   # Número de características de entrada\n",
    "hidden_size = 50  # Tamaño de la capa oculta\n",
    "num_layers = 2    # Número de capas LSTM\n",
    "output_size = 1   # Número de características de salida (en este caso, el precio de cierre)\n",
    "\n",
    "# Crear el modelo\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Traning model Try 1: With 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/datapath-mlops-mike/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([64, 4])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/anaconda3/envs/datapath-mlops-mike/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([20, 4])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 16.7894\n",
      "Epoch [2/50], Loss: 18.5285\n",
      "Epoch [3/50], Loss: 19.9328\n",
      "Epoch [4/50], Loss: 18.2754\n",
      "Epoch [5/50], Loss: 16.1228\n",
      "Epoch [6/50], Loss: 17.9610\n",
      "Epoch [7/50], Loss: 14.8300\n",
      "Epoch [8/50], Loss: 16.0894\n",
      "Epoch [9/50], Loss: 12.5965\n",
      "Epoch [10/50], Loss: 15.9026\n",
      "Epoch [11/50], Loss: 15.2565\n",
      "Epoch [12/50], Loss: 10.8868\n",
      "Epoch [13/50], Loss: 11.1081\n",
      "Epoch [14/50], Loss: 8.3635\n",
      "Epoch [15/50], Loss: 9.0330\n",
      "Epoch [16/50], Loss: 6.4391\n",
      "Epoch [17/50], Loss: 3.9665\n",
      "Epoch [18/50], Loss: 4.5360\n",
      "Epoch [19/50], Loss: 2.8014\n",
      "Epoch [20/50], Loss: 2.4265\n",
      "Epoch [21/50], Loss: 1.8758\n",
      "Epoch [22/50], Loss: 0.9108\n",
      "Epoch [23/50], Loss: 0.7739\n",
      "Epoch [24/50], Loss: 0.7933\n",
      "Epoch [25/50], Loss: 0.7103\n",
      "Epoch [26/50], Loss: 0.7648\n",
      "Epoch [27/50], Loss: 0.3409\n",
      "Epoch [28/50], Loss: 0.5853\n",
      "Epoch [29/50], Loss: 0.6043\n",
      "Epoch [30/50], Loss: 0.6267\n",
      "Epoch [31/50], Loss: 0.4456\n",
      "Epoch [32/50], Loss: 0.5171\n",
      "Epoch [33/50], Loss: 0.7421\n",
      "Epoch [34/50], Loss: 0.4340\n",
      "Epoch [35/50], Loss: 0.4713\n",
      "Epoch [36/50], Loss: 0.4560\n",
      "Epoch [37/50], Loss: 0.6546\n",
      "Epoch [38/50], Loss: 0.6051\n",
      "Epoch [39/50], Loss: 0.5157\n",
      "Epoch [40/50], Loss: 0.4531\n",
      "Epoch [41/50], Loss: 0.5697\n",
      "Epoch [42/50], Loss: 0.5371\n",
      "Epoch [43/50], Loss: 0.3830\n",
      "Epoch [44/50], Loss: 0.3170\n",
      "Epoch [45/50], Loss: 0.3587\n",
      "Epoch [46/50], Loss: 0.3187\n",
      "Epoch [47/50], Loss: 0.3675\n",
      "Epoch [48/50], Loss: 0.4730\n",
      "Epoch [49/50], Loss: 0.3286\n",
      "Epoch [50/50], Loss: 0.4817\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Configurar la función de pérdida y el optimizador\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "train_size = int(len(data_tensor) * 0.8)\n",
    "test_size = len(data_tensor) - train_size\n",
    "train_data = data_tensor[:train_size]\n",
    "test_data = data_tensor[train_size:]\n",
    "\n",
    "# Crear DataLoader para los datos de entrenamiento y prueba\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Entrenamiento\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Los datos deben tener forma (batch_size, seq_len, input_size)\n",
    "        batch = batch.unsqueeze(1)  # Añadir dimensión de secuencia\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch[:, -1, :])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Evaluate model after train with 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/datapath-mlops-mike/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([37, 4])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Evaluación\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.unsqueeze(1)\n",
    "        outputs = model(batch)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(batch[:, -1, :])\n",
    "    \n",
    "    # Convertir las listas a tensores\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    \n",
    "    # Calcular métricas de evaluación\n",
    "    mse = criterion(predictions, actuals)\n",
    "    print(f'Test MSE: {mse.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training model Try 2: 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3672\n",
      "Epoch [2/100], Loss: 0.6559\n",
      "Epoch [3/100], Loss: 0.6063\n",
      "Epoch [4/100], Loss: 0.3469\n",
      "Epoch [5/100], Loss: 0.3492\n",
      "Epoch [6/100], Loss: 0.4440\n",
      "Epoch [7/100], Loss: 0.3522\n",
      "Epoch [8/100], Loss: 0.2509\n",
      "Epoch [9/100], Loss: 0.2384\n",
      "Epoch [10/100], Loss: 0.3018\n",
      "Epoch [11/100], Loss: 0.2348\n",
      "Epoch [12/100], Loss: 0.4031\n",
      "Epoch [13/100], Loss: 0.2122\n",
      "Epoch [14/100], Loss: 0.3108\n",
      "Epoch [15/100], Loss: 0.1669\n",
      "Epoch [16/100], Loss: 0.1702\n",
      "Epoch [17/100], Loss: 0.2322\n",
      "Epoch [18/100], Loss: 0.1403\n",
      "Epoch [19/100], Loss: 0.3811\n",
      "Epoch [20/100], Loss: 0.3165\n",
      "Epoch [21/100], Loss: 0.2490\n",
      "Epoch [22/100], Loss: 0.2771\n",
      "Epoch [23/100], Loss: 0.1002\n",
      "Epoch [24/100], Loss: 0.1751\n",
      "Epoch [25/100], Loss: 0.1883\n",
      "Epoch [26/100], Loss: 0.1206\n",
      "Epoch [27/100], Loss: 0.1487\n",
      "Epoch [28/100], Loss: 0.1255\n",
      "Epoch [29/100], Loss: 0.1321\n",
      "Epoch [30/100], Loss: 0.1306\n",
      "Epoch [31/100], Loss: 0.0636\n",
      "Epoch [32/100], Loss: 0.1309\n",
      "Epoch [33/100], Loss: 0.1176\n",
      "Epoch [34/100], Loss: 0.1019\n",
      "Epoch [35/100], Loss: 0.0452\n",
      "Epoch [36/100], Loss: 0.1263\n",
      "Epoch [37/100], Loss: 0.0951\n",
      "Epoch [38/100], Loss: 0.0919\n",
      "Epoch [39/100], Loss: 0.0529\n",
      "Epoch [40/100], Loss: 0.0677\n",
      "Epoch [41/100], Loss: 0.0554\n",
      "Epoch [42/100], Loss: 0.0385\n",
      "Epoch [43/100], Loss: 0.0351\n",
      "Epoch [44/100], Loss: 0.0720\n",
      "Epoch [45/100], Loss: 0.0978\n",
      "Epoch [46/100], Loss: 0.0549\n",
      "Epoch [47/100], Loss: 0.0324\n",
      "Epoch [48/100], Loss: 0.0192\n",
      "Epoch [49/100], Loss: 0.0405\n",
      "Epoch [50/100], Loss: 0.0448\n",
      "Epoch [51/100], Loss: 0.0274\n",
      "Epoch [52/100], Loss: 0.0547\n",
      "Epoch [53/100], Loss: 0.0310\n",
      "Epoch [54/100], Loss: 0.0746\n",
      "Epoch [55/100], Loss: 0.0449\n",
      "Epoch [56/100], Loss: 0.0672\n",
      "Epoch [57/100], Loss: 0.0376\n",
      "Epoch [58/100], Loss: 0.0447\n",
      "Epoch [59/100], Loss: 0.0477\n",
      "Epoch [60/100], Loss: 0.0479\n",
      "Epoch [61/100], Loss: 0.0163\n",
      "Epoch [62/100], Loss: 0.0278\n",
      "Epoch [63/100], Loss: 0.0252\n",
      "Epoch [64/100], Loss: 0.0402\n",
      "Epoch [65/100], Loss: 0.0431\n",
      "Epoch [66/100], Loss: 0.0565\n",
      "Epoch [67/100], Loss: 0.0405\n",
      "Epoch [68/100], Loss: 0.0300\n",
      "Epoch [69/100], Loss: 0.0317\n",
      "Epoch [70/100], Loss: 0.0162\n",
      "Epoch [71/100], Loss: 0.0451\n",
      "Epoch [72/100], Loss: 0.0194\n",
      "Epoch [73/100], Loss: 0.0286\n",
      "Epoch [74/100], Loss: 0.0304\n",
      "Epoch [75/100], Loss: 0.0419\n",
      "Epoch [76/100], Loss: 0.0218\n",
      "Epoch [77/100], Loss: 0.0234\n",
      "Epoch [78/100], Loss: 0.0589\n",
      "Epoch [79/100], Loss: 0.0282\n",
      "Epoch [80/100], Loss: 0.0408\n",
      "Epoch [81/100], Loss: 0.0373\n",
      "Epoch [82/100], Loss: 0.0403\n",
      "Epoch [83/100], Loss: 0.0451\n",
      "Epoch [84/100], Loss: 0.0315\n",
      "Epoch [85/100], Loss: 0.0337\n",
      "Epoch [86/100], Loss: 0.0225\n",
      "Epoch [87/100], Loss: 0.0192\n",
      "Epoch [88/100], Loss: 0.0343\n",
      "Epoch [89/100], Loss: 0.0249\n",
      "Epoch [90/100], Loss: 0.0295\n",
      "Epoch [91/100], Loss: 0.0401\n",
      "Epoch [92/100], Loss: 0.0434\n",
      "Epoch [93/100], Loss: 0.0326\n",
      "Epoch [94/100], Loss: 0.0374\n",
      "Epoch [95/100], Loss: 0.0347\n",
      "Epoch [96/100], Loss: 0.0392\n",
      "Epoch [97/100], Loss: 0.0525\n",
      "Epoch [98/100], Loss: 0.0297\n",
      "Epoch [99/100], Loss: 0.0185\n",
      "Epoch [100/100], Loss: 0.0195\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Los datos deben tener forma (batch_size, seq_len, input_size)\n",
    "        batch = batch.unsqueeze(1)  # Añadir dimensión de secuencia\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch[:, -1, :])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate model after train with 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.1156\n"
     ]
    }
   ],
   "source": [
    "# Evaluación\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        batch = batch.unsqueeze(1)\n",
    "        outputs = model(batch)\n",
    "        predictions.append(outputs)\n",
    "        actuals.append(batch[:, -1, :])\n",
    "    \n",
    "    # Convertir las listas a tensores\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    actuals = torch.cat(actuals, dim=0)\n",
    "    \n",
    "    # Calcular métricas de evaluación\n",
    "    mse = criterion(predictions, actuals)\n",
    "    print(f'Test MSE: {mse.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/model_lstm.pkl']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model\n",
    "joblib.dump(model, PATH_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dataset de prueba para obtener una nueva predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevos datos transformados:\n",
      "          0         1         2         3\n",
      "0  2.530892  2.704617  2.671731  2.564649\n",
      "1  2.902864  2.777207  2.727185  2.754498\n",
      "2  2.517846  2.586126  2.571966  2.547804\n",
      "3  2.718088  2.618013  2.598847  2.738327\n"
     ]
    }
   ],
   "source": [
    "# Cargar el transformador guardado (para uso futuro)\n",
    "loaded_transformer = joblib.load(BOX_COX_TRANSFORMER_PATH)\n",
    "\n",
    "# Nuevos datos (debe tener el mismo número de características que el transformador espera)\n",
    "new_info = pd.DataFrame({\n",
    "    'Open': [12, 17.16, 11.85, 14.37],\n",
    "    'High': [14.3, 15.34, 12.75, 13.15],\n",
    "    'Low': [13.3, 14.01, 12.11, 12.42],\n",
    "    'Close': [12.41, 14.9, 12.21, 14.67]\n",
    "})\n",
    "\n",
    "# Aplicar la transformación a los nuevos datos\n",
    "new_info_transformed = pd.DataFrame(loaded_transformer.transform(new_info))\n",
    "\n",
    "print(\"Nuevos datos transformados:\")\n",
    "print(new_info_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos en array tensor el dataset de\n",
    "new_info_transformed_numpy = np.array(new_info_transformed)\n",
    "new_info_transformed_numpy_tensor = torch.tensor(new_info_transformed_numpy, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5309, 2.7046, 2.6717, 2.5646],\n",
       "        [2.9029, 2.7772, 2.7272, 2.7545],\n",
       "        [2.5178, 2.5861, 2.5720, 2.5478],\n",
       "        [2.7181, 2.6180, 2.5988, 2.7383]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_info_transformed_numpy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del tensor de entrada: torch.Size([1, 4, 4])\n",
      "   Feature1  Feature2  Feature3  Feature4\n",
      "0  2.530892  2.704617  2.671731  2.564649\n",
      "1  2.902864  2.777207  2.727185  2.754498\n",
      "2  2.517847  2.586126  2.571966  2.547804\n",
      "3  2.718088  2.618013  2.598846  2.738327\n"
     ]
    }
   ],
   "source": [
    "# Tensor proporcionado\n",
    "#tensor = torch.tensor([\n",
    "#    [2.5309, 2.7046, 2.6717, 2.5646],\n",
    "#    [2.9029, 2.7772, 2.7272, 2.7545],\n",
    "#    [2.5178, 2.5861, 2.5720, 2.5478],\n",
    "#    [2.7181, 2.6180, 2.5988, 2.7383]\n",
    "#], dtype=torch.float32)\n",
    "\n",
    "# Añadir dimensión de batch_size\n",
    "# tensor = tensor.unsqueeze(0)  # Forma final: [1, 4, 4]\n",
    "tensor = new_info_transformed_numpy_tensor.unsqueeze(0)\n",
    "\n",
    "# Verificar la forma del tensor\n",
    "print(f'Forma del tensor de entrada: {tensor.shape}')\n",
    "\n",
    "# Convertir el tensor a DataFrame si lo necesitas\n",
    "df = pd.DataFrame(tensor.squeeze(0).numpy(), columns=['Feature1', 'Feature2', 'Feature3', 'Feature4'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones:\n",
      "[[5.6601396]]\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo guardado\n",
    "model_cargado = joblib.load('../models/model_lstm.pkl')\n",
    "\n",
    "\n",
    "# Asegurarse de que el modelo esté en modo evaluación\n",
    "model_cargado.eval()\n",
    "\n",
    "# Tensor de entrada (ya ajustado a [batch_size, sequence_length, input_size])\n",
    "# Realizar la predicción\n",
    "with torch.no_grad():\n",
    "    #predicciones = model(tensor)\n",
    "    predicciones = model_cargado(tensor)\n",
    "\n",
    "# Convertir las predicciones a NumPy para una visualización más fácil\n",
    "predicciones_numpy = predicciones.numpy()\n",
    "\n",
    "# Mostrar las predicciones\n",
    "print(\"Predicciones:\")\n",
    "print(predicciones_numpy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datapath-mlops-mike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
